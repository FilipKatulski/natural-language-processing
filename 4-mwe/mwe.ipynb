{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiword expressions identification and extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "from random import sample\n",
    "from collections import Counter\n",
    "import numpy as np  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_files = \"ustawy\"\n",
    "\n",
    "def read_normalized_documents(): \n",
    "    file_names = os.listdir(path_to_files)\n",
    "    return {\n",
    "        name: _normalize_document(_read_document(name, path_to_files))\n",
    "        for name in file_names\n",
    "        if name.endswith(\".txt\")\n",
    "    }\n",
    "\n",
    "def _read_document(name: str, path: str):\n",
    "    with open(os.path.join(path, name), 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def _normalize_document(document: str):\n",
    "    return re.sub(r\"\\s+\", \" \", document).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pl_core_news_sm\")  # had to be downloaded separately with \"python3 -m spacy download pl_core_news_sm\"\n",
    "tokenizer = nlp.tokenizer\n",
    "documents = read_normalized_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "2001_994.txt: ['994', 'przez', 'art', 'dnia', 'przeciwko', 'wchodzi', 'zgromadzenie', 'przyjętej', 'poz', 'konwencji'] <class 'list'>\n",
      "1999_700.txt: [',', '.', 'życie', 'nr', '1999', '1999', 'ogłoszenia', ',', 'poz', 'grudnia'] <class 'list'>\n",
      "1998_717.txt: ['33', 'rozstrzygnięcie', ',', 'o', 'samorządową', 'przysługuje', 'sprawie', '§', 'i', 'czynnej'] <class 'list'>\n",
      "1996_776.txt: ['dostępna', 'w', 'wprowadzenia', 'o', '.', 'skargi', '2', 'dokonać', 'wynosi', 'aby'] <class 'list'>\n",
      "2004_1055.txt: ['19a', 'pierwszego', ';', 'niezbędnym', 'urbaniście', ',', 'zawodowych', 'krajowa', 'polskiej', 'życie'] <class 'list'>\n",
      "1999_527.txt: ['.', 'od', 'siedzibę', '.', 'przewodniczący', '1998', 'nr', 'w', 'nr', 'w'] <class 'list'>\n",
      "2004_2783.txt: ['i', 'r', ',', 'nawzajem', 'budynku', ':', 'przyjęcia', '.', 'tych', 'użytkowej'] <class 'list'>\n",
      "2001_1197.txt: ['gmin', 'tonę', 'zaspokojenia', 'prawnych', 'zarządzającego', 'skład', 'użytkowanie', ')', 'art', 'portem'] <class 'list'>\n",
      "2003_844.txt: ['powołujący', 'umarza', 'sądów', 'od', 'uchwałę', '6', 'okręgowe', '.', 'podejrzenie', '.'] <class 'list'>\n",
      "2001_1378.txt: ['dnia', ')', ',', 'poz', 'i', 'mowa', '157', '.', 'z', 'lb'] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "tokenized_documents = {name: [t.text for t in tokenizer(text)] for name, text in documents.items()}\n",
    "print(type(tokenized_documents))\n",
    "for name, tokens in sample(list(tokenized_documents.items()), 10):\n",
    "    print(f\"{name}: {sample(tokens, 10)}\", type(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute bigram counts of downcased tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> [(' ', 'dz.u'), ('dz.u', '.'), ('.', 'z'), ('z', '2001'), ('2001', 'r'), ('r', '.'), ('.', 'nr'), ('nr', '81'), ('81', ','), (',', 'poz'), ('poz', '.'), ('.', '874'), ('874', 'ustawa'), ('ustawa', 'z'), ('z', 'dnia'), ('dnia', '21'), ('21', 'czerwca'), ('czerwca', '2001'), ('2001', 'r'), ('r', '.'), ('.', 'o'), ('o', 'zmianie'), ('zmianie', 'ustawy'), ('ustawy', 'o'), ('o', 'państwowej'), ('państwowej', 'straży'), ('straży', 'pożarnej'), ('pożarnej', 'art'), ('art', '.'), ('.', '1'), ('1', '.'), ('.', 'w'), ('w', 'ustawie'), ('ustawie', 'z'), ('z', 'dnia'), ('dnia', '24'), ('24', 'sierpnia'), ('sierpnia', '1991'), ('1991', 'r'), ('r', '.'), ('.', 'o'), ('o', 'państwowej'), ('państwowej', 'straży'), ('straży', 'pożarnej'), ('pożarnej', '('), ('(', 'dz.u'), ('dz.u', '.'), ('.', 'nr'), ('nr', '88'), ('88', ','), (',', 'poz'), ('poz', '.'), ('.', '400'), ('400', 'z'), ('z', '1992'), ('1992', 'r'), ('r', '.'), ('.', 'nr'), ('nr', '21'), ('21', ','), (',', 'poz'), ('poz', '.'), ('.', '86'), ('86', 'i'), ('i', 'nr'), ('nr', '54'), ('54', ','), (',', 'poz'), ('poz', '.'), ('.', '254'), ('254', ','), (',', 'z'), ('z', '1994'), ('1994', 'r'), ('r', '.'), ('.', 'nr'), ('nr', '53'), ('53', ','), (',', 'poz'), ('poz', '.'), ('.', '214'), ('214', ','), (',', 'z'), ('z', '1995'), ('1995', 'r'), ('r', '.'), ('.', 'nr'), ('nr', '4'), ('4', ','), (',', 'poz'), ('poz', '.'), ('.', '17'), ('17', 'i'), ('i', 'nr'), ('nr', '34'), ('34', ','), (',', 'poz'), ('poz', '.'), ('.', '163'), ('163', ',')]\n"
     ]
    }
   ],
   "source": [
    "bigrams = [\n",
    "    (f, s)\n",
    "    for tokens in tokenized_documents.values()\n",
    "    for (f, s) in zip(tokens[:-1], tokens[1:])\n",
    "]\n",
    "print(type(bigrams),bigrams[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('art', '.'), 83778),\n",
       " (('ust', '.'), 53552),\n",
       " (('poz', '.'), 45198),\n",
       " ((',', 'poz'), 43188),\n",
       " (('.', '1'), 39927),\n",
       " (('-', '-'), 36547),\n",
       " (('r', '.'), 33010),\n",
       " (('w', 'art'), 32042),\n",
       " ((',', 'o'), 29926),\n",
       " (('mowa', 'w'), 28471)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate bigrams count\n",
    "\n",
    "bigrams_counts = Counter(bigrams)\n",
    "\n",
    "bigrams_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discard bigrams containing characters other than letters. Make sure that you discard the invalid entries after computing the bigram counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('w', 'art'), 32042),\n",
       " (('mowa', 'w'), 28471),\n",
       " (('w', 'ust'), 23557),\n",
       " (('o', 'których'), 13884),\n",
       " (('których', 'mowa'), 13857),\n",
       " (('otrzymuje', 'brzmienie'), 9553),\n",
       " (('z', 'dnia'), 9527),\n",
       " (('o', 'którym'), 9184),\n",
       " (('którym', 'mowa'), 9171),\n",
       " (('do', 'spraw'), 8715)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def isalpha(s):\n",
    "    for letter in s:\n",
    "        if not letter.isalpha():  # is this characted alphanumeric?\n",
    "            return False\n",
    "    return True \n",
    "\n",
    "\n",
    "bigrams_alpha_total = Counter({x: count for x, count in bigrams_counts.items() if isalpha(x[0]) and isalpha(x[1])})\n",
    "\n",
    "bigrams_alpha_total.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use pointwise mutual information to compute the measure for all pairs of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PMI for 2 elements:\n",
    "```\n",
    "log2(p(a,b) / ( p(a) * p(b) )) =  log2(p(a,b)) - log2( p(a) + p(b) )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> [' ', 'dz.u', '.', 'z', '2001', 'r', '.', 'nr', '81', ',', 'poz', '.', '874', 'ustawa', 'z', 'dnia', '21', 'czerwca', '2001', 'r', '.', 'o', 'zmianie', 'ustawy', 'o', 'państwowej', 'straży', 'pożarnej', 'art', '.', '1', '.', 'w', 'ustawie', 'z', 'dnia', '24', 'sierpnia', '1991', 'r', '.', 'o', 'państwowej', 'straży', 'pożarnej', '(', 'dz.u', '.', 'nr', '88', ',', 'poz', '.', '400', 'z', '1992', 'r', '.', 'nr', '21', ',', 'poz', '.', '86', 'i', 'nr', '54', ',', 'poz', '.', '254', ',', 'z', '1994', 'r', '.', 'nr', '53', ',', 'poz', '.', '214', ',', 'z', '1995', 'r', '.', 'nr', '4', ',', 'poz', '.', '17', 'i', 'nr', '34', ',', 'poz', '.', '163']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('w', 201200),\n",
       " ('i', 90006),\n",
       " ('art', 83804),\n",
       " ('z', 82438),\n",
       " ('o', 64776),\n",
       " ('do', 60732),\n",
       " ('ust', 53636),\n",
       " ('na', 50643),\n",
       " ('się', 45886),\n",
       " ('lub', 45800)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate total number of words for word's probability, for this I have to calculate unigrams:\n",
    "\n",
    "unigrams = [\n",
    "    f\n",
    "    for tokens in tokenized_documents.values()\n",
    "    for f in tokens\n",
    "]\n",
    "print(type(unigrams),unigrams[0:100])\n",
    "\n",
    "unigrams_alpha_total = Counter({x: count for x, count in Counter(unigrams).items() if isalpha(x)})\n",
    "unigrams_alpha_total.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 3566806\n"
     ]
    }
   ],
   "source": [
    "total = sum(unigrams_alpha_total.values())\n",
    "print(\"Total:\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(word_occ, total):\n",
    "    return word_occ / total\n",
    "\n",
    "def pmi2(word1, word2, total):\n",
    "    word1_occ = unigrams_alpha_total[word1]\n",
    "    word2_occ = unigrams_alpha_total[word2]\n",
    "    together_occ = bigrams_alpha_total[(word1, word2)]\n",
    "    return np.log2(p(together_occ, total)) - np.log2( p(word1_occ, total)) - np.log2(p(word2_occ, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_total_pmi = {bigram: pmi2(bigram[0], bigram[1], total=total) for bigram, _  in bigrams_alpha_total.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sort the word pairs according to that measure in the descending order and determine top 10 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('doktorów', 'habilitowanych'), 21.76620131850449),\n",
       " (('pionową', 'ścianę'), 21.76620131850449),\n",
       " (('inte', 'gracyjnymi'), 21.76620131850449),\n",
       " (('techno', 'logii'), 21.76620131850449),\n",
       " (('usprawnianie', 'zaburzonych'), 21.76620131850449),\n",
       " (('przepro', 'wadza'), 21.76620131850449),\n",
       " (('gałki', 'ocznej'), 21.76620131850449),\n",
       " (('stępkę', 'położono'), 21.76620131850449),\n",
       " (('wybuchła', 'wojna'), 21.76620131850449),\n",
       " (('dało', 'pożytecznego'), 21.76620131850449)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(bigrams_total_pmi).most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Filter bigrams with number of occurrences lower than 5. Determine top 10 entries for the remaining dataset (>=5 occurrences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ręcznego', 'miotacza'), 19.444273223617127),\n",
       " (('stajnią', 'wyścigową'), 19.444273223617127),\n",
       " (('świeckie', 'przygotowujące'), 19.444273223617127),\n",
       " (('klęskami', 'żywiołowymi'), 19.444273223617127),\n",
       " (('obcowania', 'płciowego'), 19.444273223617127),\n",
       " (('nietykalność', 'cielesną'), 19.444273223617127),\n",
       " (('grzegorz', 'schetyna'), 19.444273223617127),\n",
       " (('młynki', 'młotkowe'), 19.444273223617127),\n",
       " (('młyny', 'kulowe'), 19.444273223617127),\n",
       " (('najnowszych', 'zdobyczy'), 19.444273223617127)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_filtered_pmi = {bigram: pmi2(bigram[0], bigram[1], total=total) for bigram, count in bigrams_alpha_total.items() if count >= 5}\n",
    "bigrams_pmi_top10 = Counter(bigrams_filtered_pmi).most_common()[:10]\n",
    "bigrams_pmi_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Use KRNNT or Clarin-PL API(https://ws.clarin-pl.eu/tager.shtml) to tag and lemmatize the corpus.\n",
    "\n",
    "Used Clarin-PL API with Morfeusz2, bulk upload of 'ustawy.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import tqdm\n",
    "\n",
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing tagged acts: 100%|██████████| 1179/1179 [03:48<00:00,  5.17it/s]\n"
     ]
    }
   ],
   "source": [
    "tag_tokenized_acts = {}\n",
    "\n",
    "path_to_results = Path(\"Wynik\")\n",
    "n_acts = len(list(path_to_results.iterdir()))\n",
    "\n",
    "for act in tqdm.tqdm(path_to_results.iterdir(), desc=\"Tokenizing tagged acts\", total=n_acts):\n",
    "    act_id = act.stem\n",
    "    content = act.read_text(encoding=\"utf8\")\n",
    "    tree = ET.fromstring(text=content)\n",
    "    tag_tokens = []\n",
    "    for tok in tree.iter(\"tok\"):\n",
    "        lex = tok.find(\"lex\")\n",
    "        base = lex.find(\"base\").text\n",
    "        ctag = lex.find(\"ctag\").text\n",
    "        tag_token = f\"{base.lower()}:{ctag.split(':')[0]}\"\n",
    "        tag_tokens.append(tag_token)\n",
    "    tag_tokenized_acts[act_id] = tag_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using the tagged corpus compute bigram statistic for the tokens containing: a. lemmatized, downcased word b. morphosyntactic category of the word (subst, fin, adj, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clarin_bigrams = [\n",
    "    (f, s)\n",
    "    for tag_tokens in tag_tokenized_acts.values()\n",
    "    for (f, s) in zip(tag_tokens[:-1], tag_tokens[1:])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for real words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word(token: str):\n",
    "    parts = token.split(\":\")\n",
    "    if len(parts) != 2:\n",
    "        return False\n",
    "    else:\n",
    "        return isalpha(parts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('art:ign', '.:interp'), 83779),\n",
       " (('usta:subst', '.:interp'), 53553),\n",
       " (('poz:ign', '.:interp'), 45222),\n",
       " ((',:interp', 'poz:ign'), 43191),\n",
       " (('.:interp', '1:num'), 39987),\n",
       " (('-:interp', '-:interp'), 36548),\n",
       " (('r:ign', '.:interp'), 33029),\n",
       " (('w:prep', 'art:ign'), 32044),\n",
       " ((',:interp', 'o:prep'), 29926),\n",
       " (('o:prep', 'który:adj'), 28656)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_bigrams_counter = Counter(clarin_bigrams)\n",
    "tag_bigrams_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clarin_bigrams_alpha = (\n",
    "    (f, s)\n",
    "    for tag_tokens in tag_tokenized_acts.values()\n",
    "    for (f, s) in zip(tag_tokens[:-1], tag_tokens[1:])\n",
    "    if is_word(f) and is_word(s)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('w:prep', 'art:ign'), 32044),\n",
       " (('o:prep', 'który:adj'), 28656),\n",
       " (('który:adj', 'mowa:subst'), 28538),\n",
       " (('mowa:subst', 'w:prep'), 28473),\n",
       " (('w:prep', 'usta:subst'), 23557),\n",
       " (('z:prep', 'dzień:subst'), 11360),\n",
       " (('otrzymywać:fin', 'brzmienie:subst'), 10536),\n",
       " (('określony:adj', 'w:prep'), 10240),\n",
       " (('do:prep', 'sprawa:subst'), 8718),\n",
       " (('ustawa:subst', 'z:prep'), 8625)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clarin_bigrams_alpha_total = Counter(clarin_bigrams_alpha)\n",
    "clarin_bigrams_alpha_total.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('w:prep', 202951),\n",
       " ('i:conj', 90044),\n",
       " ('z:prep', 87991),\n",
       " ('art:ign', 83805),\n",
       " ('o:prep', 64809),\n",
       " ('do:prep', 60768),\n",
       " ('usta:subst', 53641),\n",
       " ('na:prep', 50657),\n",
       " ('który:adj', 49382),\n",
       " ('się:qub', 45887)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clarin_unigrams_alpha_total = Counter(\n",
    "    tag_token for tag_tokens in tag_tokenized_acts.values() for tag_token in tag_tokens if is_word(tag_token))\n",
    "\n",
    "clarin_unigrams_alpha_total.most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating PMIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clarin_pmi2(word1, word2, total):\n",
    "    word1_occ = clarin_unigrams_alpha_total[word1]\n",
    "    word2_occ = clarin_unigrams_alpha_total[word2]\n",
    "    together_occ = clarin_bigrams_alpha_total[(word1, word2)]\n",
    "    return np.log2(p(together_occ, total)) - np.log2(p(word1_occ, total)) - np.log2(p(word2_occ, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clarin total: 3578037\n"
     ]
    }
   ],
   "source": [
    "clarin_total = sum(clarin_unigrams_alpha_total.values())\n",
    "print(\"Clarin total:\", clarin_total)\n",
    "clarin_bigrams_total_pmi = {bigram: clarin_pmi2(bigram[0], bigram[1], total=clarin_total) \\\n",
    "    for bigram, _  in clarin_bigrams_alpha_total.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('atrakcyjny:adj', 'turystycznie:adv'), 21.77073687550676),\n",
       " (('niepeł:ign', 'nosprawności:ign'), 21.77073687550676),\n",
       " (('dzonej:ign', 'obdukcja:subst'), 21.77073687550676),\n",
       " (('inwestycy:ign', 'jnych:ign'), 21.77073687550676),\n",
       " (('okr:ign', 'eśl:ign'), 21.77073687550676),\n",
       " (('pos:ign', 'taca:subst'), 21.77073687550676),\n",
       " (('zwi:ign', 'ązk:ign'), 21.77073687550676),\n",
       " (('ier:ign', 'ają:ign'), 21.77073687550676),\n",
       " (('ają:ign', 'cyc:subst'), 21.77073687550676),\n",
       " (('potasowyc:ign', 'atmosfe:ign'), 21.77073687550676)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(clarin_bigrams_total_pmi).most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compute the same statistics as for the non-lemmatized words (i.e. PMI) and print top-10 entries with at least 5 occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('młynek:subst', 'młotkowy:adj'), 19.448808780619398),\n",
       " (('grzegorz:subst', 'schetyna:ign'), 19.448808780619398),\n",
       " (('teryto:ign', 'rialnego:ign'), 19.448808780619398),\n",
       " (('pasta:subst', 'emulsyjny:adj'), 19.185774374785606),\n",
       " (('chrom:subst', 'sześciowartościowy:adj'), 19.185774374785606),\n",
       " (('odpowiedzieć:fin', 'dzialności:ign'), 19.185774374785606),\n",
       " (('adam:subst', 'mickiewicz:subst'), 19.185774374785606),\n",
       " (('łańcuchowa:subst', 'rozszczepienie:subst'), 19.185774374785606),\n",
       " (('młyn:subst', 'kulowy:adj'), 18.963381953449158),\n",
       " (('piotrek:subst', 'trybunalski:adj'), 18.963381953449158)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clarin_bigrams_filtered_pmi = {bigram: clarin_pmi2(bigram[0], bigram[1], total=clarin_total) \\\n",
    "    for bigram, count in clarin_bigrams_alpha_total.items() if count >= 5}\n",
    "clarin_bigrams_pmi_top10 = Counter(clarin_bigrams_filtered_pmi).most_common()[:10]\n",
    "clarin_bigrams_pmi_top10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compute trigram counts for both corpora and perform the same filtering.\n",
    "\n",
    "### A. The corpus tokenized by SpaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('o', 'których', 'mowa'), 13856),\n",
       " (('których', 'mowa', 'w'), 13806),\n",
       " (('mowa', 'w', 'ust'), 13474),\n",
       " (('mowa', 'w', 'art'), 12311),\n",
       " (('o', 'którym', 'mowa'), 9169),\n",
       " (('którym', 'mowa', 'w'), 9147),\n",
       " (('o', 'której', 'mowa'), 5510),\n",
       " (('której', 'mowa', 'w'), 5487),\n",
       " (('w', 'drodze', 'rozporządzenia'), 4685),\n",
       " (('właściwy', 'do', 'spraw'), 4620)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_alpha = [\n",
    "    (f, s, t)\n",
    "    for tokens in tokenized_documents.values()\n",
    "    for (f, s, t) in zip(tokens[:-2], tokens[1:-1],tokens[2:])\n",
    "    if isalpha(f) and isalpha(s) and isalpha(t)\n",
    "]\n",
    "\n",
    "trigrams_alpha_total = Counter(trigrams_alpha)\n",
    "trigrams_alpha_total.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. The corpus lemmatized by CLARIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('o:prep', 'który:adj', 'mowa:subst'), 28535),\n",
       " (('który:adj', 'mowa:subst', 'w:prep'), 28442),\n",
       " (('mowa:subst', 'w:prep', 'usta:subst'), 13474),\n",
       " (('mowa:subst', 'w:prep', 'art:ign'), 12311),\n",
       " (('ustawa:subst', 'z:prep', 'dzień:subst'), 8589),\n",
       " (('właściwy:adj', 'do:prep', 'sprawa:subst'), 7966),\n",
       " (('minister:subst', 'właściwy:adj', 'do:prep'), 7888),\n",
       " (('w:prep', 'droga:subst', 'rozporządzenie:subst'), 4751),\n",
       " (('zastępować:fin', 'się:qub', 'wyraz:subst'), 3653),\n",
       " (('w:prep', 'ustawa:subst', 'z:prep'), 3646)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clarin_trigrams_alpha = [\n",
    "    (f, s, t)\n",
    "    for tokens in tag_tokenized_acts.values()\n",
    "    for (f, s, t) in zip(tokens[:-2], tokens[1:-1],tokens[2:])\n",
    "    if is_word(f) and is_word(s) and is_word(t)\n",
    "]\n",
    "\n",
    "clarin_trigrams_alpha_total = Counter(clarin_trigrams_alpha)\n",
    "clarin_trigrams_alpha_total.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Use PMI (with 5 occurrence threshold) to compute top 10 results for the trigrams. Devise a method for computing the values, based on the results for bigrams.\n",
    "\n",
    "PMI for 3 elements:\n",
    "```\n",
    "log2( p(a,b,c) / ( p(a) * p(b) * p(c) ) ) = log2(p(a,b,c)) - log2(p(a)) - log2(p(b)) - log2(p(c))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi3(word1, word2, word3, total):\n",
    "    word1_occ = unigrams_alpha_total[word1]\n",
    "    word2_occ = unigrams_alpha_total[word2]\n",
    "    word3_occ = unigrams_alpha_total[word3]\n",
    "    together_occ = trigrams_alpha_total[(word1, word2, word3)]\n",
    "    return np.log2(p(together_occ, total)) - np.log2( p(word1_occ, total)) - np.log2(p(word2_occ, total)) - np.log2(p(word3_occ, total)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
