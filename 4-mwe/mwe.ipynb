{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiword expressions identification and extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "from random import sample\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_files = \"ustawy\"\n",
    "\n",
    "def read_normalized_documents(): \n",
    "    file_names = os.listdir(path_to_files)\n",
    "    return {\n",
    "        name: _normalize_document(_read_document(name, path_to_files))\n",
    "        for name in file_names\n",
    "        if name.endswith(\".txt\")\n",
    "    }\n",
    "\n",
    "def _read_document(name: str, path: str):\n",
    "    with open(os.path.join(path, name), 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def _normalize_document(document: str):\n",
    "    return re.sub(r\"\\s+\", \" \", document).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipe that converts lemmas to lower case:\n",
    "def lower_case_lemmas(doc) :\n",
    "    for token in doc :\n",
    "        token.lemma_ = token.lemma_.lower()\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pl_core_news_sm\")  # had to be downloaded separately with \"python3 -m spacy download pl_core_news_sm\"\n",
    "tokenizer = nlp.tokenizer\n",
    "documents = read_normalized_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "2003_1068.txt: [',', 'o', 'mowa', 'mowa', '1', 'ustawy', 'górnictwa', '.', 'nr', 'poz'] <class 'list'>\n",
      "2001_1645.txt: ['polskiej', 'przywilejach', 'ratyfikacji', 'jądrowych', '2004', 'i', 'protokołu', 'przywilejach', 'o', '.'] <class 'list'>\n",
      "1996_347.txt: ['.', 'z', '.', '.', 'z', 'środków', '1', 'prezydent', 'dnia', '.'] <class 'list'>\n",
      "1994_601.txt: ['art', '1998', 'poz', '.', '1998', 'dnia', '.', 'lipca', 'nr', 'art'] <class 'list'>\n",
      "2003_1759.txt: ['.', 'osobie', 'nr', ')', 'postoju', 'kontroli', 'ruchu', 'na', ')', 'i'] <class 'list'>\n",
      "1997_348.txt: ['których', 'karę', 'nr', 'z', 'organizacje', 'nie', 'i', 'energetycznych', 'w', 'rozporządzenia'] <class 'list'>\n",
      "2004_76.txt: ['.', 'określone', ':', 'zaplanowano', 'brzmienie', ',', '873', ')', 'do', 'lipca'] <class 'list'>\n",
      "1997_805.txt: ['bez', '198', 'w', 'posiadają', '11', 'cmentarzu', 'nagrobków', 'ustawa', 'miejsca', 'ogłoszenia'] <class 'list'>\n",
      "2001_1086.txt: ['wykonywania', 'sprzedaż', ')', 'umowy', 'według', 's.a', 'koleje', '741', 'art', 'nr'] <class 'list'>\n",
      "2003_1610.txt: ['1', ',', 'poz', 'straży', ',', '1217', 'poz', 'zmiany', '3', ','] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "tokenized_documents = {name: [t.text for t in tokenizer(text)] for name, text in documents.items()}\n",
    "print(type(tokenized_documents))\n",
    "for name, tokens in sample(list(tokenized_documents.items()), 10):\n",
    "    print(f\"{name}: {sample(tokens, 10)}\", type(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercase:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute bigram counts of downcased tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> [(' ', 'dz.u'), ('dz.u', '.'), ('.', 'z'), ('z', '2001'), ('2001', 'r'), ('r', '.'), ('.', 'nr'), ('nr', '81'), ('81', ','), (',', 'poz'), ('poz', '.'), ('.', '874'), ('874', 'ustawa'), ('ustawa', 'z'), ('z', 'dnia'), ('dnia', '21'), ('21', 'czerwca'), ('czerwca', '2001'), ('2001', 'r'), ('r', '.'), ('.', 'o'), ('o', 'zmianie'), ('zmianie', 'ustawy'), ('ustawy', 'o'), ('o', 'państwowej'), ('państwowej', 'straży'), ('straży', 'pożarnej'), ('pożarnej', 'art'), ('art', '.'), ('.', '1'), ('1', '.'), ('.', 'w'), ('w', 'ustawie'), ('ustawie', 'z'), ('z', 'dnia'), ('dnia', '24'), ('24', 'sierpnia'), ('sierpnia', '1991'), ('1991', 'r'), ('r', '.'), ('.', 'o'), ('o', 'państwowej'), ('państwowej', 'straży'), ('straży', 'pożarnej'), ('pożarnej', '('), ('(', 'dz.u'), ('dz.u', '.'), ('.', 'nr'), ('nr', '88'), ('88', ','), (',', 'poz'), ('poz', '.'), ('.', '400'), ('400', 'z'), ('z', '1992'), ('1992', 'r'), ('r', '.'), ('.', 'nr'), ('nr', '21'), ('21', ','), (',', 'poz'), ('poz', '.'), ('.', '86'), ('86', 'i'), ('i', 'nr'), ('nr', '54'), ('54', ','), (',', 'poz'), ('poz', '.'), ('.', '254'), ('254', ','), (',', 'z'), ('z', '1994'), ('1994', 'r'), ('r', '.'), ('.', 'nr'), ('nr', '53'), ('53', ','), (',', 'poz'), ('poz', '.'), ('.', '214'), ('214', ','), (',', 'z'), ('z', '1995'), ('1995', 'r'), ('r', '.'), ('.', 'nr'), ('nr', '4'), ('4', ','), (',', 'poz'), ('poz', '.'), ('.', '17'), ('17', 'i'), ('i', 'nr'), ('nr', '34'), ('34', ','), (',', 'poz'), ('poz', '.'), ('.', '163'), ('163', ',')]\n"
     ]
    }
   ],
   "source": [
    "bigrams = [\n",
    "    (f, s)\n",
    "    for tokens in tokenized_documents.values()\n",
    "    for (f, s) in zip(tokens[:-1], tokens[1:])\n",
    "]\n",
    "print(type(bigrams),bigrams[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('obowiązku', 'zapobiegania'), 1), (('sposób', 'stronniczy'), 2), (('wymagają', 'wyłączenia'), 1), (('518', 'otrzymuje'), 1), (('kontrakty', 'z'), 1), (('każdym', 'województwie'), 1), (('uwagę', 'stanowiska'), 3), (('o', 'wprowadzonych'), 4), (('na', 'kwasach'), 1), (('przepisów', 'działalność'), 1), (('obcym', 'nowożytnym'), 1), (('tym', 'kary'), 1), (('wymagające', 'oznakowania'), 1), (('obliczając', 'należne'), 2), (('składa', 'prokuratorowi'), 1)]\n"
     ]
    }
   ],
   "source": [
    "# Calculate bigrams count\n",
    "\n",
    "freq_list = Counter(bigrams)\n",
    "\n",
    "print(sample(list(freq_list.items()), 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PMI:\n",
    "```\n",
    "log(p(a,b) / ( p(a) * p(b) ))\n",
    "```\n",
    "programmatically:\n",
    "```\n",
    "def pmi(word1, word2, unigram_freq, bigram_freq):\n",
    "  prob_word1 = unigram_freq[word1] / float(sum(unigram_freq.values()))\n",
    "  prob_word2 = unigram_freq[word2] / float(sum(unigram_freq.values()))\n",
    "  prob_word1_word2 = bigram_freq[\" \".join([word1, word2])] / float(sum(bigram_freq.values()))\n",
    "  return math.log(prob_word1_word2/float(prob_word1*prob_word2),2) \n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
